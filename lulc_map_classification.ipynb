{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LULC Maps of Mumbai: Classification and Georeferencing\n",
    "\n",
    "Georeferencing and classifying pixels of land use maps of Mumbai\n",
    "\n",
    "By Harsh Vardhan Pachisia\n",
    "\n",
    "Jan - Feb 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat new environment. Run this on command line\n",
    "# conda create -n lulc numpy rasterio matplotlib pandas opencv jupyter scipy\n",
    "# conda activate lulc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up paths\n",
    "user = 'harsh'\n",
    "system = 'cpu'\n",
    "if user == 'tom':\n",
    "    root = '/Users/tombearpark/Library/CloudStorage/Dropbox/india_mortality/data'\n",
    "    # Need to add path to the input PDF, I didn't see it on dropbox\n",
    "    pdf_path = root + 'lulc/lulc_maps.pdf'\n",
    "    output_folder = root + 'lucl/lulc_maps_tifs'\n",
    "elif user == 'harsh' and system == 'cpu':\n",
    "    #convert to dropbox equivalent\n",
    "    root = '/Users/Harsh/Desktop/01_Projects/Mumbai_Flooding/'\n",
    "    pdf_path = root + '/lulc_maps.pdf'\n",
    "    #convert to dropbox equivalent\n",
    "    output_folder = root\n",
    "elif user== 'harsh' and system == 'dropbox':\n",
    "    #convert to dropbox equivalent\n",
    "    root = '/Users/harshpachisia/Library/CloudStorage/Dropbox/india_mortality/data'\n",
    "    pdf_path = root + '/lulc_maps.pdf'\n",
    "    #convert to dropbox equivalent\n",
    "    output_folder = root + 'lucl/lulc_maps_tifs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from   rasterio.transform import from_origin\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.ndimage import distance_transform_edt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to TIFF Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_images = False\n",
    "if convert_images:\n",
    "    from pdf2image import convert_from_path\n",
    "\n",
    "    # Convert PDF to a list of images\n",
    "    images = convert_from_path(pdf_path, dpi=600)\n",
    "\n",
    "    years = [1972, 1977, 1991, 1998, 1999, 2000, 2001, 2002, 2003, 2005, 2009, 2011,\n",
    "             2013, '2014_1', '2014_2', 2015]\n",
    "\n",
    "    # Save each image as a TIFF\n",
    "    for i, image in enumerate(images):\n",
    "        filename = f'lulc_map_{years[i]}.tiff'\n",
    "        image.save(f'{output_folder}{filename}', 'TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Georeference and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big picture steps:\n",
    "1. Georeference the original image in QGIS\n",
    "2. Classify the image of a map of land use in Mumbai (different colors with different land use)\n",
    "3. Store that classification as a tif, define the boundaries\n",
    "4. Import the classification data (classified image which has the geographical boundaries of Mumbai) onto QGIS\n",
    "5. Import other data layers like Mumbai pincodes in QGIS\n",
    "6. Do analysis of differences in pincodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Georeferencing\n",
    "For georeferencing the image (which means assigning real-world coordinates to the image), this generally involves:\n",
    "\n",
    "1. Having control points that tie image pixel coordinates to geographic coordinates (latitude and longitude, for example).\n",
    "2. Using these points to define a spatial reference system and transformation parameters.\n",
    "3. Applying these parameters to the image to create a georeferenced image, typically saved as a GeoTIFF.\n",
    "\n",
    "This was done in QGIS using the georeferencer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually do this based on  https://www.rapidtables.com/web/color/RGB_Color.html#color-picker\n",
    "legend_colors = {\n",
    "    # adding a background for the legend- to be removed later\n",
    "    (227, 0, 5): 'Built up land',\n",
    "    (13, 114, 78): 'Forest plantation',\n",
    "    (2, 255, 198): 'Mangrove',\n",
    "    (86, 255, 7): 'Vegetation',\n",
    "    (255, 255, 5): 'Crop land/Grass land',\n",
    "    (255, 254, 190): 'Fallow land',\n",
    "    (209, 253, 123): 'Aquatic vegetation',\n",
    "    (132, 1, 170): 'Marshy/Swampy land',\n",
    "    (191, 255, 231): 'Mudflats',\n",
    "    (200, 200, 200): 'Sandy Area',\n",
    "    (252, 116, 223): 'Barren Land',\n",
    "    (0, 77, 170): 'Waterbodies',\n",
    "    (119, 0, 77): 'Others (Salt pans)',\n",
    "    (255, 255, 255): 'Background'\n",
    "}\n",
    "\n",
    "# creating a color map for mapping the categories and pixels\n",
    "# This is there to map to easier, more recognizable colors as compared to what\n",
    "# was present in the original legend (for future work done for these maps)\n",
    "color_map ={\n",
    " 'Built up land': (255, 0, 0),\n",
    " 'Crop land/Grass land': (255,255,0),\n",
    " 'Fallow land': (255, 254, 190),\n",
    " 'Vegetation': (0,255,0),\n",
    " 'Forest plantation': (0,100,0),\n",
    " 'Mangrove': (0,255,255),\n",
    " 'Aquatic vegetation': (50,205,50),\n",
    " 'Marshy/Swampy land': (128,0,128),\n",
    " 'Mudflats': (173,216,230),\n",
    " 'Sandy Area': (255,250,240),\n",
    " 'Barren Land': (255,192,203),\n",
    " 'Waterbodies': (0,0,128),\n",
    " 'Others (Salt pans)': (119, 0, 77),\n",
    " 'Background': (250,250,250)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Classify Colors Based on a Legend:\n",
    "1. Load the Image: Read the TIFF image into a format that allows access individual pixel colors.\n",
    "\n",
    "2. Define the Color Legend: Define a mapping from the colors in the legend to their respective classes. Possible dictionary where keys are color tuples (in RGB format) and values are the corresponding class names or identifiers.\n",
    "\n",
    "3. Process Each Pixel: Iterate over each pixel in the image and replace its color with the corresponding class identifier based on legend mapping.\n",
    "\n",
    "4. Handling Ambiguity: Exact color matches will be rare due to variations in shading, lighting, or image quality. Need a method to handle colors that don't exactly match your legend colors, like finding the nearest color in your legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(geo_image_path):\n",
    "    '''\n",
    "    Function that loads the georeferenced image and saves the image dimensions\n",
    "    to be used in later functions\n",
    "    \n",
    "    '''\n",
    "    # Load the georeferenced image\n",
    "\n",
    "    with rasterio.open(geo_image_path) as src:\n",
    "        image = src.read()\n",
    "        # Pixel sizes in the x and y dimensions\n",
    "        pixel_size_x, pixel_size_y = src.res\n",
    "        # Upper left coordinates\n",
    "        upper_left_x, upper_left_y = src.transform * (0, 0)\n",
    "        crs = src.crs\n",
    "\n",
    "    return image, (pixel_size_x, pixel_size_y), (upper_left_x, upper_left_y), crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pixels(pixels, legend_colors, tolerance=0):\n",
    "    '''\n",
    "    function to classify pixels using opencv with a tolerance\n",
    "\n",
    "    Returns: matrix of classified pixels and counts of matches\n",
    "    '''\n",
    "    classified_pixels = np.empty(pixels.shape[:2], dtype=object)\n",
    "    match_counts = {}\n",
    "    for color, classification in legend_colors.items():\n",
    "        lower_bound = np.array(color) - tolerance\n",
    "        upper_bound = np.array(color) + tolerance\n",
    "        mask = np.all(np.logical_and(pixels >= lower_bound, \n",
    "                                     pixels <= upper_bound), axis=-1)\n",
    "        match_counts[classification] = np.sum(mask)\n",
    "\n",
    "        classified_pixels[mask] = classification\n",
    "\n",
    "    return classified_pixels, match_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling the None (black pixels) issue\n",
    "We have a bunch of None's (black pixels) where no category was chosen. Hence, to get around this issue we assume that since the color of the pixel (given image quality was quite poor), the pixel next to the unclassified pixel must have a high chance of being that category. The idea is to search for the nearest pixel with a classification and assign that classification to the unclassified pixel. This can be done using a spatial search algorithm.\n",
    "\n",
    "For example, an unclassified pixel next to built up area will be classified as built up area- we use a technique like the k-nearest neighbors algorithm, but optimizing it for this specific task. The Scipy library provides functions that can help with this, which can be used to find the nearest classified pixel. Check all the closest pixels, and give it the value of a very close pixel which also has a similar color?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_distance_matrix(rgb_image, classified_pixels, indices):\n",
    "    \"\"\"\n",
    "    Helper function to compute a matrix of color distances between unclassified pixels and \n",
    "    their nearest classified neighbors.\n",
    "    \"\"\"\n",
    "    unclassified_pixels = np.argwhere(classified_pixels == None)\n",
    "    nearest_indices = (indices[0][unclassified_pixels[:, 0], unclassified_pixels[:, 1]],\n",
    "                       indices[1][unclassified_pixels[:, 0], unclassified_pixels[:, 1]])\n",
    "\n",
    "    # Extract RGB values for nearest classified pixels\n",
    "    nearest_colors = rgb_image[nearest_indices[0], nearest_indices[1]]\n",
    "\n",
    "    # Extract RGB values for unclassified pixels\n",
    "    unclassified_colors = rgb_image[unclassified_pixels[:, 0], unclassified_pixels[:, 1]]\n",
    "\n",
    "    # Calculate color distances\n",
    "    return np.sqrt(np.sum((nearest_colors - unclassified_colors) ** 2, axis=1))\n",
    "\n",
    "\n",
    "def fill_unclassified_pixels(classified_pixels, rgb_image, tolerance=0):\n",
    "    '''\n",
    "    Function that fills in the unclassified pixels.\n",
    "\n",
    "    Returns a matrix of filled in pixels\n",
    "    '''\n",
    "    # Create a mask for unclassified pixels\n",
    "    unclassified_mask = classified_pixels == None\n",
    "    \n",
    "    # Compute nearest neighbor indices for unclassified pixels\n",
    "    distances, indices = distance_transform_edt(unclassified_mask, return_indices=True)\n",
    "\n",
    "    # Compute color distances\n",
    "    color_distances = color_distance_matrix(rgb_image, classified_pixels, indices)\n",
    "    \n",
    "    # Fill unclassified pixels\n",
    "    filled_pixels = classified_pixels.copy()\n",
    "    unclassified_positions = np.argwhere(unclassified_mask)\n",
    "    for i, pos in enumerate(unclassified_positions):\n",
    "        if color_distances[i] <= tolerance:\n",
    "            filled_pixels[tuple(pos)] = classified_pixels[tuple(indices[:, pos[0], pos[1]])]\n",
    "        else:\n",
    "            # Fallback to the nearest spatial classification\n",
    "            filled_pixels[tuple(pos)] = classified_pixels[tuple(indices[:, pos[0], pos[1]])]\n",
    "\n",
    "    return filled_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(filled_classified_pixels):\n",
    "    '''\n",
    "    Function that converts the classified pixels to numeric to get back\n",
    "    into a georeferenced image format and maps it\n",
    "\n",
    "    Returns: A matrix of numeric classified pixels\n",
    "    '''\n",
    "    unique_classes = np.unique(\n",
    "        filled_classified_pixels[filled_classified_pixels != None])\n",
    "    class_to_id_map = {cls: i for i, cls in enumerate(unique_classes, start=1)}\n",
    "    class_to_id_map[None] = 0\n",
    "    numeric_classified_pixels = np.vectorize(\n",
    "        class_to_id_map.get)(filled_classified_pixels)\n",
    "    \n",
    "    # convert to int8 since QGIS wont take int64\n",
    "    numeric_classified_pixels = numeric_classified_pixels.astype(np.uint8)\n",
    "    \n",
    "    return numeric_classified_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(output_path, numeric_classified_pixels, shape, crs, transform):\n",
    "    '''\n",
    "    Function that saves the georeferenced, classified image\n",
    "    '''\n",
    "    with rasterio.open(\n",
    "        output_path, 'w', driver='GTiff',\n",
    "        height=shape[0], width=shape[1], count=1, dtype=numeric_classified_pixels.dtype,\n",
    "        crs=crs, transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(numeric_classified_pixels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Over years where maps are available\n",
    "years = [2005, 2009, 2013, 2014, 2015]\n",
    "\n",
    "for year in years:\n",
    "    geo_image_path = f\"{root}/lulc/georeferenced_maps/{year}_geocoded.tif\" # georeferenced image\n",
    "    output_path = f\"{root}/lulc/final_maps/lulc_{year}.tif\" # output path for classified, georeferenced map\n",
    "\n",
    "    # Load image\n",
    "    image, pixel_sizes, upper_left_coords, crs = load_image(geo_image_path)\n",
    "    rgb_image = np.dstack((image[0], image[1], image[2]))\n",
    "    \n",
    "    # classify pixels\n",
    "    classified_pixels, match_counts = classify_pixels(rgb_image,\n",
    "                                                       legend_colors,\n",
    "                                                         tolerance=20)\n",
    "    #deal with unclassified pixels\n",
    "    filled_classified_pixels = fill_unclassified_pixels(classified_pixels, \n",
    "                                                        rgb_image)\n",
    "    #conver to numeric\n",
    "    numeric_classified_pixels = convert_to_numeric(filled_classified_pixels)\n",
    "    transform = from_origin(upper_left_coords[0], upper_left_coords[1], \n",
    "                            pixel_sizes[0], pixel_sizes[1])\n",
    "    # save the image\n",
    "    save_image(output_path, numeric_classified_pixels, \n",
    "               rgb_image.shape[:2], crs, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import into QGIS\n",
    "Steps to take:\n",
    "1. Load OSM boundaries and zoom into Mumbai\n",
    "2. After importing georeferenced and classified image, apply style in properties.\n",
    "3. Blur out background. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
